{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment Setup\n",
    "* Action space (4 high level skill): pre-pick, pick, pre-place, place.\n",
    "* Using rosbag to record each skill start position and end position sequentially.\n",
    "* Save trajectories as np.ndarray `demo_traj` (Shape: m trajectories by n position by 3 dimension).\n",
    "\n",
    "\n",
    "## Discussion:\n",
    "1. How many trajectories should we collect? one or more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Human Demonstration\n",
    "* Action space (4 high level skill): pre-pick, pick, pre-place, place.\n",
    "* Using rosbag to record each skill start position and end position sequentially.\n",
    "* Save trajectories as np.ndarray `demo_traj` (Shape: m trajectories by n position by 3 dimension).\n",
    "\n",
    "\n",
    "## Discussion:\n",
    "1. How many trajectories should we collect? one or more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collect Transition Tuple\n",
    "\n",
    "## 2.1 Pseudocode\n",
    "\n",
    "* Robot repeatly runs `demo_traj` to collect transition tuple with form $<s, z, a, r, s', z'>$, with contact mode using function `perturbation`\n",
    "* Zip up the\n",
    "\n",
    "## 2.2 Class \n",
    "\n",
    "* Class `Agent`\n",
    "    * `act`\n",
    "    * `learn`\n",
    "    * `demo_record`\n",
    "    * `execute_demo`\n",
    "    * Class `Exp_Buffer`\n",
    "    * Class `Region_Cluster`\n",
    "    * Class `Recovery`\n",
    "    \n",
    "* Class `Enviornment`\n",
    "    * `Perturbation`\n",
    "    * `get_reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \n",
    "        self.demo_act = {}\n",
    "        self.num_of_demo_goal = {}        \n",
    "        \n",
    "        self.experience = Exp_Buffer()\n",
    "        \n",
    "        self.act = {}\n",
    "        self.state_size = 0\n",
    "        self.action_size = 0\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "#   To record a task demonstration skill with several goal states, \n",
    "#   i.e. the goal 3D position of each initial skill.\n",
    "    def demo_record(self):\n",
    "        self.num_of_demo_goal = \n",
    "        self.demo_act =\n",
    "\n",
    "        return\n",
    "\n",
    "#   Return the demo action list to robot (then robot execute the demo to get experience)\n",
    "    def get_demo_act_list(self):\n",
    "        demo_act_list = self.demo_act\n",
    "        return demo_act_list\n",
    "        \n",
    "    def exp_record(self,episode_list):\n",
    "        for exp_tuple in episode_list:\n",
    "            self.experience.append(exp_tuple)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Buffer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        self.experience = namedtuple(\"Experience\", \\\n",
    "                                 field_name = [\"s\", \"z\", \"a\", \"r\", \"next_s\", \"next_z\", \"done\"])  \n",
    "        \n",
    "        \n",
    "    def add(self, state, contact, action, reward, next_state, next_contact, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, contact, action, reward, next_state, next_contact, done))\n",
    "        self.memory.append(e)\n",
    "\n",
    "#  Not done yet\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Region_Cluster(experience_list,action_list):\n",
    "    \n",
    "    def __init__(self):\n",
    "#        experience_list is a list of namedtuples: [(s,z,a,r,s',z'),(s,z,a,r,s',z'),...]\n",
    "        self.e_list = experience_list\n",
    "        self.a_list = action_list\n",
    "        \n",
    "    def learn_state_distribution_component(self):\n",
    "        \n",
    "        for action in self.a_list:\n",
    "            region_psi_set = []\n",
    "            exp_set = self.extract_exp_with_same_act(action)\n",
    "            region_phy_set = self.cluster()\n",
    "            \n",
    "#     Get a list of state with same act\n",
    "    def extract_exp_with_same_act(self,act):\n",
    "        for e in self.e_list:\n",
    "            if e.action == act:\n",
    "                \n",
    "            \n",
    "    \n",
    "    def cluster():\n",
    "        \n",
    "    \n",
    "    def DBSCAN(self, distance_type):\n",
    "        \n",
    "    def B_distance(self):\n",
    "        \n",
    "    def states_distance(se;f):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0fda2e17825a>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0fda2e17825a>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    contact_mode = result_s/\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Env():\n",
    "\n",
    "    def __init__():\n",
    "        \n",
    "        self.m\n",
    "        \n",
    "#   Reset the environment to initial state.\n",
    "\n",
    "    def reset():\n",
    "        \n",
    "    def get_reward():\n",
    "        \n",
    "        return r\n",
    "    \n",
    "    def perturbation(self, delta_d):\n",
    "        contact_mode = []\n",
    "        \n",
    "        current_s = ROS_current_pos()\n",
    "        \n",
    "        desired_1 = current_s + delta_d\n",
    "        desired_2 = current_s - delta_d\n",
    "        desired_list = [desired_1,desired_2]\n",
    "        \n",
    "        for desired_s in desired_list:\n",
    "        \n",
    "            ROS_move_to(desired_s)\n",
    "            result_s = ROS_current_pos() - current_s    \n",
    "            contact_s = result_s/delta_d        \n",
    "            contact_mode.append(contact_s)\n",
    "            ROS_move_to(current_s)\n",
    "        \n",
    "        return contact_mode\n",
    "    \n",
    "    def execute_demo_act(self,execute_demo_act_list):\n",
    "        \n",
    "        episode_record= []\n",
    "        cache_exp_tuple = ()\n",
    "        \n",
    "        state = ROS_current_pos\n",
    "        contact = perturbation()\n",
    "        \n",
    "        for i, act in enumerate(execute_demo_act_list):\n",
    "\n",
    "#           Noise move means adding the Gaussian noise to the goal position of an action, \n",
    "#           to model the mechanical or control error.\n",
    "\n",
    "            ROS_noise_move_to(act)\n",
    "            \n",
    "            next_state = ROS_current_pos\n",
    "            next_contact = perturbation()\n",
    "            reward = get_reward()\n",
    "            \n",
    "            exp_tuple = cache_exp_tuple = (state, contact, reward, act, next_state, next_contact)\n",
    "            episode_record.append(exp_tuple)\n",
    "            \n",
    "            state = next_state\n",
    "            contact = next_contact\n",
    "\n",
    "#       [(s,z,a,r,s',z',),(s,z,a,r,s',z',),(s,z,a,r,s',z',),...]\n",
    "        return episode_record\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = Agent()\n",
    "robot = Env()\n",
    "\n",
    "# How many times does the robot execute the human demonstration\n",
    "repeat_times = \n",
    "\n",
    "agent.demo_record()\n",
    "\n",
    "demo_list = agent.get_demo_act_list()\n",
    "\n",
    "for i in repeat_times:\n",
    "    episode_record = robot.execute_demo_act(demo_list)\n",
    "    agent.exp_record (episode_record) \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
