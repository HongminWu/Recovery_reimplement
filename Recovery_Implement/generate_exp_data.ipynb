{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: the original demonstration skills amount is:6 \n",
      "\n",
      "Env: The demonstration is:\n",
      "point 0: [10, 5]\n",
      "point 1: [20, 20]\n",
      "point 2: [30, 40]\n",
      "point 3: [40, 35]\n",
      "point 4: [45, 45]\n",
      "point 5: [50, 55]\n",
      "\n",
      "\n",
      "Env: The final goal position is:[50, 55]\n",
      "Agent: agent has recorded the demonstration as its original skills: OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])]) \n",
      "\n",
      "OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])])\n",
      "Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-12.619848578410494, next_state=array([9.6692901 , 4.13551118, 0.15345452, 0.28941199, 0.28191778,\n",
      "       0.17857635]), done=False)\n"
     ]
    }
   ],
   "source": [
    "import Recovery_RL_Agent\n",
    "import env_robot\n",
    "import numpy as np\n",
    "# import torch\n",
    "import region\n",
    "\n",
    "\n",
    "robot = env_robot.Env(dim=2)\n",
    "agent = Recovery_RL_Agent.Agent(dim=2)\n",
    "\n",
    "demo_goal =  [[10,5],[20,20],[30,40],[40,35],[45,45],[50,55]]\n",
    "robot.demonstration(demo_goal)\n",
    "demo_act_dict = agent.demo_record(demo_goal)\n",
    "\n",
    "repeat_times = 2\n",
    "\n",
    "for i in range(0,repeat_times):\n",
    "    # executing the demo action and restore experience tuples in agent\n",
    "    episode_record,_ = robot.execute_separate_demo_act(demo_act_dict)\n",
    "    agent.exp_record(episode_record)\n",
    "    # Reset env, back to start point\n",
    "    robot.test_reset()\n",
    "\n",
    "\n",
    "exp_tuple_test = agent.get_exp_list()\n",
    "# for e in exp_tuple_test:\n",
    "#     print(e.state)\n",
    "experience_list = exp_tuple_test\n",
    "action_dict = demo_act_dict\n",
    "print(action_dict)\n",
    "print(experience_list[0])\n",
    "\n",
    "\n",
    "\n",
    "# obj = region.Region_Cluster(experience_list, action_dict)\n",
    "# r_s = obj.learn_state_region()\n",
    "# for i, mean in enumerate(r_s):\n",
    "#     print(\"Region {}: {}\".format(i, mean[0]) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
