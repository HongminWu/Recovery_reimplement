{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: the original demonstration skills amount is:6 \n",
      "\n",
      "Env: The demonstration is:\n",
      "point 0: [10, 5]\n",
      "point 1: [20, 20]\n",
      "point 2: [30, 40]\n",
      "point 3: [40, 35]\n",
      "point 4: [45, 45]\n",
      "point 5: [50, 55]\n",
      "\n",
      "\n",
      "Env: The final goal position is:[50, 55]\n",
      "Agent: agent has recorded the demonstration as its original skills: OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])]) \n",
      "\n",
      "OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])])\n",
      "[Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-14.160562087777372, next_state=array([10.60608398,  5.17320377,  0.15299155,  0.32293585,  0.29669273,\n",
      "        0.2518864 ]), done=False), Experience(state=array([10.60608398,  5.17320377,  0.15299155,  0.32293585,  0.29669273,\n",
      "        0.2518864 ]), action={'1': [20, 20]}, reward=-21.326635559736815, next_state=array([20.08776932, 20.20479066,  0.55655983,  0.63591465,  0.5844643 ,\n",
      "        0.40751494]), done=False), Experience(state=array([20.08776932, 20.20479066,  0.55655983,  0.63591465,  0.5844643 ,\n",
      "        0.40751494]), action={'2': [30, 40]}, reward=-28.642189998107398, next_state=array([30.23844349, 41.80730691,  0.64662561,  0.84263532,  0.81086419,\n",
      "        0.6602239 ]), done=False), Experience(state=array([30.23844349, 41.80730691,  0.64662561,  0.84263532,  0.81086419,\n",
      "        0.6602239 ]), action={'3': [40, 35]}, reward=-16.37616738312879, next_state=array([42.01338867, 34.9090474 ,  1.08616019,  1.02287278,  1.13777584,\n",
      "        0.98907877]), done=False), Experience(state=array([42.01338867, 34.9090474 ,  1.08616019,  1.02287278,  1.13777584,\n",
      "        0.98907877]), action={'4': [45, 45]}, reward=-10.671336147768121, next_state=array([45.22291576, 45.08629413,  1.37889102,  1.26370214,  1.28406037,\n",
      "        1.18074394]), done=True), Experience(state=array([45.22291576, 45.08629413,  1.37889102,  1.26370214,  1.28406037,\n",
      "        1.18074394]), action={'5': [50, 55]}, reward=-9.168319517252973, next_state=array([48.54194282, 53.6327637 ,  1.57453672,  1.56677433,  1.52018079,\n",
      "        1.51462927]), done=True), Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-13.219196505447163, next_state=array([9.95033595, 4.72683894, 0.1953814 , 0.22409631, 0.29932529,\n",
      "       0.29711232]), done=False), Experience(state=array([9.95033595, 4.72683894, 0.1953814 , 0.22409631, 0.29932529,\n",
      "       0.29711232]), action={'1': [20, 20]}, reward=-22.202887417122724, next_state=array([19.60774699, 20.50889985,  0.50859788,  0.45285482,  0.39881016,\n",
      "        0.39776664]), done=False), Experience(state=array([19.60774699, 20.50889985,  0.50859788,  0.45285482,  0.39881016,\n",
      "        0.39776664]), action={'2': [30, 40]}, reward=-27.923262296208513, next_state=array([31.81889387, 40.31678165,  0.80148921,  0.708288  ,  0.73854509,\n",
      "        0.65846049]), done=False), Experience(state=array([31.81889387, 40.31678165,  0.80148921,  0.708288  ,  0.73854509,\n",
      "        0.65846049]), action={'3': [40, 35]}, reward=-12.714288524150051, next_state=array([39.59735952, 33.1227152 ,  1.01794262,  0.95832838,  1.01651101,\n",
      "        1.00887979]), done=False), Experience(state=array([39.59735952, 33.1227152 ,  1.01794262,  0.95832838,  1.01651101,\n",
      "        1.00887979]), action={'4': [45, 45]}, reward=-14.106692306621659, next_state=array([44.77641095, 46.24430787,  1.20414162,  1.19269152,  1.35820868,\n",
      "        1.336954  ]), done=True), Experience(state=array([44.77641095, 46.24430787,  1.20414162,  1.19269152,  1.35820868,\n",
      "        1.336954  ]), action={'5': [50, 55]}, reward=-9.865348437097179, next_state=array([51.02077457, 53.8819137 ,  1.4214788 ,  1.45045727,  1.53759366,\n",
      "        1.49793555]), done=True)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Experience' object has no attribute 'next_contact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5daf494fa19b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegion_Cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mr_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_state_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m# for i, mean in enumerate(r_s):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#     print(\"Region {}: {}\".format(i, mean[0]) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/Documents/program/Recovery_reimplement/Recovery_Implement/region.pyc\u001b[0m in \u001b[0;36mlearn_state_region\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0msame_act_exp_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_exp_with_same_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             region_psi_set = self.cluster(same_act_exp_set, component='next', \\\n\u001b[0;32m---> 65\u001b[0;31m                 distance_type = 'states_dist')\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# convert set to list for iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mlist_region_psi_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion_psi_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/Documents/program/Recovery_reimplement/Recovery_Implement/region.pyc\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, input_set, component, distance_type)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mtrain_set_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_sz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             cluster = clustering.DBSCAN(train_set_ns, eps=5, minpts=3, \\\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# TODO:eps threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jim/Documents/program/Recovery_reimplement/Recovery_Implement/region.pyc\u001b[0m in \u001b[0;36mextract_sz\u001b[0;34m(self, input_set, component)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_contact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Experience' object has no attribute 'next_contact'"
     ]
    }
   ],
   "source": [
    "import Recovery_RL_Agent\n",
    "import env_robot\n",
    "import numpy as np\n",
    "# import torch\n",
    "import region\n",
    "\n",
    "\n",
    "robot = env_robot.Env(dim=2)\n",
    "agent = Recovery_RL_Agent.Agent(dim=2)\n",
    "\n",
    "demo_goal =  [[10,5],[20,20],[30,40],[40,35],[45,45],[50,55]]\n",
    "robot.demonstration(demo_goal)\n",
    "demo_act_dict = agent.demo_record(demo_goal)\n",
    "\n",
    "repeat_times = 2\n",
    "\n",
    "for i in range(0,repeat_times):\n",
    "    # executing the demo action and restore experience tuples in agent\n",
    "    episode_record,_ = robot.execute_separate_demo_act(demo_act_dict)\n",
    "    agent.exp_record(episode_record)\n",
    "    # Reset env, back to start point\n",
    "    robot.test_reset()\n",
    "\n",
    "\n",
    "exp_tuple_test = agent.get_exp_list()\n",
    "# for e in exp_tuple_test:\n",
    "#     print(e.state)\n",
    "experience_list = exp_tuple_test\n",
    "action_dict = demo_act_dict\n",
    "print(action_dict)\n",
    "print(experience_list)\n",
    "\n",
    "\n",
    "\n",
    "obj = region.Region_Cluster(experience_list, action_dict)\n",
    "r_s = obj.learn_state_region()\n",
    "# for i, mean in enumerate(r_s):\n",
    "#     print(\"Region {}: {}\".format(i, mean[0]) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
