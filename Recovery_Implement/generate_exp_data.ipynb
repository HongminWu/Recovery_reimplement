{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: the original demonstration skills amount is:6 \n",
      "\n",
      "Env: The demonstration is:\n",
      "point 0: [10, 5]\n",
      "point 1: [20, 20]\n",
      "point 2: [30, 40]\n",
      "point 3: [40, 35]\n",
      "point 4: [45, 45]\n",
      "point 5: [50, 55]\n",
      "\n",
      "\n",
      "Env: The final goal position is:[50, 55]\n",
      "Agent: agent has recorded the demonstration as its original skills: OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])]) \n",
      "\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[10.65609993  4.55748777  0.31301322  0.36952227  0.18402324  0.24908362]\n",
      "[20.1424475  20.34333412  0.50472557  0.54008907  0.53729985  0.50147035]\n",
      "[29.92275979 41.27446061  0.73399126  0.81425297  0.67923386  0.75919689]\n",
      "[41.03094801 34.65644494  1.00855115  1.11632482  0.94027773  1.02958802]\n",
      "[43.89545957 45.64294078  1.30292724  1.29486469  1.26512616  1.24074794]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[8.87774474 3.23007039 0.10977084 0.21472224 0.25124499 0.14200267]\n",
      "[20.94716506 20.36410461  0.46227295  0.43542408  0.50230124  0.52002256]\n",
      "[28.40923861 38.85771673  0.68812295  0.70166637  0.68897677  0.59801861]\n",
      "[39.53771096 35.40952936  1.03848657  1.03588123  0.9807233   0.92858643]\n",
      "[45.09135986 45.5315071   1.11163072  1.25599336  1.25868288  1.14585504]\n",
      "OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])])\n",
      "[Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-13.907744291634547, next_state=array([10.65609993,  4.55748777,  0.31301322,  0.36952227,  0.18402324,\n",
      "        0.24908362]), done=False), Experience(state=array([10.65609993,  4.55748777,  0.31301322,  0.36952227,  0.18402324,\n",
      "        0.24908362]), action={'1': [20, 20]}, reward=-22.10032982143506, next_state=array([20.1424475 , 20.34333412,  0.50472557,  0.54008907,  0.53729985,\n",
      "        0.50147035]), done=False), Experience(state=array([20.1424475 , 20.34333412,  0.50472557,  0.54008907,  0.53729985,\n",
      "        0.50147035]), action={'2': [30, 40]}, reward=-27.724066325866747, next_state=array([29.92275979, 41.27446061,  0.73399126,  0.81425297,  0.67923386,\n",
      "        0.75919689]), done=False), Experience(state=array([29.92275979, 41.27446061,  0.73399126,  0.81425297,  0.67923386,\n",
      "        0.75919689]), action={'3': [40, 35]}, reward=-15.516235582153694, next_state=array([41.03094801, 34.65644494,  1.00855115,  1.11632482,  0.94027773,\n",
      "        1.02958802]), done=False), Experience(state=array([41.03094801, 34.65644494,  1.00855115,  1.11632482,  0.94027773,\n",
      "        1.02958802]), action={'4': [45, 45]}, reward=-11.353788677772037, next_state=array([43.89545957, 45.64294078,  1.30292724,  1.29486469,  1.26512616,\n",
      "        1.24074794]), done=True), Experience(state=array([43.89545957, 45.64294078,  1.30292724,  1.29486469,  1.26512616,\n",
      "        1.24074794]), action={'5': [50, 55]}, reward=-13.093579316040463, next_state=array([49.66839811, 57.39517461,  1.35708204,  1.53110865,  1.49758725,\n",
      "        1.55601185]), done=True), Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-11.33652050788444, next_state=array([8.87774474, 3.23007039, 0.10977084, 0.21472224, 0.25124499,\n",
      "       0.14200267]), done=False), Experience(state=array([8.87774474, 3.23007039, 0.10977084, 0.21472224, 0.25124499,\n",
      "       0.14200267]), action={'1': [20, 20]}, reward=-25.149836799279054, next_state=array([20.94716506, 20.36410461,  0.46227295,  0.43542408,  0.50230124,\n",
      "        0.52002256]), done=False), Experience(state=array([20.94716506, 20.36410461,  0.46227295,  0.43542408,  0.50230124,\n",
      "        0.52002256]), action={'2': [30, 40]}, reward=-23.93078712604033, next_state=array([28.40923861, 38.85771673,  0.68812295,  0.70166637,  0.68897677,\n",
      "        0.59801861]), done=False), Experience(state=array([28.40923861, 38.85771673,  0.68812295,  0.70166637,  0.68897677,\n",
      "        0.59801861]), action={'3': [40, 35]}, reward=-13.980535254067119, next_state=array([39.53771096, 35.40952936,  1.03848657,  1.03588123,  0.9807233 ,\n",
      "        0.92858643]), done=False), Experience(state=array([39.53771096, 35.40952936,  1.03848657,  1.03588123,  0.9807233 ,\n",
      "        0.92858643]), action={'4': [45, 45]}, reward=-11.545451462782287, next_state=array([45.09135986, 45.5315071 ,  1.11163072,  1.25599336,  1.25868288,\n",
      "        1.14585504]), done=True), Experience(state=array([45.09135986, 45.5315071 ,  1.11163072,  1.25599336,  1.25868288,\n",
      "        1.14585504]), action={'5': [50, 55]}, reward=-10.988475359830323, next_state=array([50.81437678, 54.91200112,  1.60996959,  1.43261769,  1.55291389,\n",
      "        1.60754564]), done=True)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Experience' object has no attribute 'next_contact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6f821a87bd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegion_Cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mr_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_state_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Region {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bourne/Recovery_reimplement/Recovery_Implement/region.pyc\u001b[0m in \u001b[0;36mlearn_state_region\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0msame_act_exp_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_exp_with_same_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             region_psi_set = self.cluster(same_act_exp_set, component='next', \\\n\u001b[0;32m---> 65\u001b[0;31m                 distance_type = 'states_dist')\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# convert set to list for iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mlist_region_psi_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion_psi_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bourne/Recovery_reimplement/Recovery_Implement/region.pyc\u001b[0m in \u001b[0;36mcluster\u001b[0;34m(self, input_set, component, distance_type)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mtrain_set_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_sz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             cluster = clustering.DBSCAN(train_set_ns, eps=5, minpts=3, \\\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# TODO:eps threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bourne/Recovery_reimplement/Recovery_Implement/region.pyc\u001b[0m in \u001b[0;36mextract_sz\u001b[0;34m(self, input_set, component)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_contact\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Experience' object has no attribute 'next_contact'"
     ]
    }
   ],
   "source": [
    "import Recovery_RL_Agent\n",
    "import env_robot\n",
    "import numpy as np\n",
    "# import torch\n",
    "import region\n",
    "\n",
    "\n",
    "robot = env_robot.Env(dim=2)\n",
    "agent = Recovery_RL_Agent.Agent(dim=2)\n",
    "\n",
    "demo_goal =  [[10,5],[20,20],[30,40],[40,35],[45,45],[50,55]]\n",
    "robot.demonstration(demo_goal)\n",
    "demo_act_dict = agent.demo_record(demo_goal)\n",
    "\n",
    "repeat_times = 2\n",
    "\n",
    "for i in range(0,repeat_times):\n",
    "    # executing the demo action and restore experience tuples in agent\n",
    "    episode_record,_ = robot.execute_separate_demo_act(demo_act_dict)\n",
    "    agent.exp_record(episode_record)\n",
    "    # Reset env, back to start point\n",
    "    robot.test_reset()\n",
    "\n",
    "\n",
    "exp_tuple_test = agent.get_exp_list()\n",
    "for e in exp_tuple_test:\n",
    "    print(e.state)\n",
    "experience_list = exp_tuple_test\n",
    "action_dict = demo_act_dict\n",
    "print(action_dict)\n",
    "print(experience_list)\n",
    "\n",
    "\n",
    "\n",
    "obj = region.Region_Cluster(experience_list, action_dict)\n",
    "r_s = obj.learn_state_region()\n",
    "for i, mean in enumerate(r_s):\n",
    "    print(\"Region {}: {}\".format(i, mean[0]) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
