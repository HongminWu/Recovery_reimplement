{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: the original demonstration skills amount is:6 \n",
      "\n",
      "Env: The demonstration is:\n",
      "point 0: [10, 5]\n",
      "point 1: [20, 20]\n",
      "point 2: [30, 40]\n",
      "point 3: [40, 35]\n",
      "point 4: [45, 45]\n",
      "point 5: [50, 55]\n",
      "\n",
      "\n",
      "Env: The final goal position is:[50, 55]\n",
      "Agent: agent has recorded the demonstration as its original skills: OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])]) \n",
      "\n",
      "OrderedDict([('0', [10, 5]), ('1', [20, 20]), ('2', [30, 40]), ('3', [40, 35]), ('4', [45, 45]), ('5', [50, 55])])\n",
      "[Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-13.58613568114229, next_state=array([10.53954412,  4.1353    ,  0.34742021,  0.26883666,  0.1307724 ,\n",
      "        0.21913072]), done=False), Experience(state=array([10.53954412,  4.1353    ,  0.34742021,  0.26883666,  0.1307724 ,\n",
      "        0.21913072]), action={'1': [20, 20]}, reward=-22.46144213727522, next_state=array([20.73574497, 19.83230887,  0.34740167,  0.42121339,  0.56793506,\n",
      "        0.5174144 ]), done=False), Experience(state=array([20.73574497, 19.83230887,  0.34740167,  0.42121339,  0.56793506,\n",
      "        0.5174144 ]), action={'2': [30, 40]}, reward=-27.31990458729049, next_state=array([29.21350285, 40.96155666,  0.77129158,  0.743883  ,  0.74417588,\n",
      "        0.75182581]), done=False), Experience(state=array([29.21350285, 40.96155666,  0.77129158,  0.743883  ,  0.74417588,\n",
      "        0.75182581]), action={'3': [40, 35]}, reward=-16.33904774806319, next_state=array([40.67911025, 33.61772605,  1.05059918,  0.97507575,  0.90995573,\n",
      "        0.99018005]), done=False), Experience(state=array([40.67911025, 33.61772605,  1.05059918,  0.97507575,  0.90995573,\n",
      "        0.99018005]), action={'4': [45, 45]}, reward=-12.246084876813216, next_state=array([45.61164686, 44.82650285,  1.2343266 ,  1.15356612,  1.19229693,\n",
      "        1.34384652]), done=True), Experience(state=array([45.61164686, 44.82650285,  1.2343266 ,  1.15356612,  1.19229693,\n",
      "        1.34384652]), action={'5': [50, 55]}, reward=-11.920175311366984, next_state=array([51.52570701, 55.17611501,  1.53576798,  1.62315035,  1.40852587,\n",
      "        1.26931874]), done=True), Experience(state=array([0., 0., 0., 0., 0., 0.]), action={'0': [10, 5]}, reward=-12.568989873494461, next_state=array([9.39715636, 4.62616935, 0.43826957, 0.32493163, 0.28569899,\n",
      "       0.23453724]), done=False), Experience(state=array([9.39715636, 4.62616935, 0.43826957, 0.32493163, 0.28569899,\n",
      "       0.23453724]), action={'1': [20, 20]}, reward=-21.678392538719123, next_state=array([20.33419901, 19.00452607,  0.49976279,  0.4905352 ,  0.47961156,\n",
      "        0.44611629]), done=False), Experience(state=array([20.33419901, 19.00452607,  0.49976279,  0.4905352 ,  0.47961156,\n",
      "        0.44611629]), action={'2': [30, 40]}, reward=-26.54068915564411, next_state=array([28.1295547 , 39.70246746,  0.78091635,  0.74177308,  0.78416007,\n",
      "        0.72250569]), done=False), Experience(state=array([28.1295547 , 39.70246746,  0.78091635,  0.74177308,  0.78416007,\n",
      "        0.72250569]), action={'3': [40, 35]}, reward=-16.871630343258964, next_state=array([41.5839024 , 35.62135662,  1.06995296,  1.01683521,  0.8688091 ,\n",
      "        1.06077962]), done=False), Experience(state=array([41.5839024 , 35.62135662,  1.06995296,  1.01683521,  0.8688091 ,\n",
      "        1.06077962]), action={'4': [45, 45]}, reward=-9.713519150579401, next_state=array([44.92383135, 44.74261361,  1.2756698 ,  1.16313836,  1.3361159 ,\n",
      "        1.30561997]), done=True), Experience(state=array([44.92383135, 44.74261361,  1.2756698 ,  1.16313836,  1.3361159 ,\n",
      "        1.30561997]), action={'5': [50, 55]}, reward=-11.412113191310109, next_state=array([50.39731367, 54.75646994,  1.48468526,  1.39627962,  1.36302101,\n",
      "        1.56901256]), done=True)]\n"
     ]
    }
   ],
   "source": [
    "import Recovery_RL_Agent\n",
    "import env_robot\n",
    "import numpy as np\n",
    "# import torch\n",
    "import region\n",
    "\n",
    "\n",
    "robot = env_robot.Env(dim=2)\n",
    "agent = Recovery_RL_Agent.Agent(dim=2)\n",
    "\n",
    "demo_goal =  [[10,5],[20,20],[30,40],[40,35],[45,45],[50,55]]\n",
    "robot.demonstration(demo_goal)\n",
    "demo_act_dict = agent.demo_record(demo_goal)\n",
    "\n",
    "repeat_times = 2\n",
    "\n",
    "for i in range(0,repeat_times):\n",
    "    # executing the demo action and restore experience tuples in agent\n",
    "    episode_record,_ = robot.execute_separate_demo_act(demo_act_dict)\n",
    "    agent.exp_record(episode_record)\n",
    "    # Reset env, back to start point\n",
    "    robot.test_reset()\n",
    "\n",
    "\n",
    "exp_tuple_test = agent.get_exp_list()\n",
    "# for e in exp_tuple_test:\n",
    "#     print(e.state)\n",
    "experience_list = exp_tuple_test\n",
    "action_dict = demo_act_dict\n",
    "print(action_dict)\n",
    "print(experience_list)\n",
    "\n",
    "\n",
    "\n",
    "obj = region.Region_Cluster(experience_list, action_dict)\n",
    "r_s = obj.learn_state_region()\n",
    "# for i, mean in enumerate(r_s):\n",
    "#     print(\"Region {}: {}\".format(i, mean[0]) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
